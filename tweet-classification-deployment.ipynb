{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Tweet Classification Deployment","metadata":{}},{"cell_type":"markdown","source":"## Import Libraries","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-15T08:12:25.414600Z","iopub.execute_input":"2021-11-15T08:12:25.415458Z","iopub.status.idle":"2021-11-15T08:12:25.441440Z","shell.execute_reply.started":"2021-11-15T08:12:25.415420Z","shell.execute_reply":"2021-11-15T08:12:25.440583Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"pip install tweepy","metadata":{"execution":{"iopub.status.busy":"2021-11-15T08:12:25.587250Z","iopub.execute_input":"2021-11-15T08:12:25.587780Z","iopub.status.idle":"2021-11-15T08:12:34.251017Z","shell.execute_reply.started":"2021-11-15T08:12:25.587702Z","shell.execute_reply":"2021-11-15T08:12:34.249886Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"import re\nimport string\n\nfrom sklearn.base import TransformerMixin, BaseEstimator\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom imblearn.under_sampling import InstanceHardnessThreshold\nfrom sklearn.svm import LinearSVC\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.naive_bayes import MultinomialNB, ComplementNB\nfrom sklearn.svm import LinearSVC\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.utils.multiclass import unique_labels\nfrom sklearn.feature_selection import SelectFromModel\n\nimport tweepy\nfrom tweepy import OAuthHandler\nfrom textblob import TextBlob\nimport joblib\nfrom joblib import load\nfrom sklearn.base import TransformerMixin, BaseEstimator\n\nfrom imblearn.pipeline import Pipeline\nimport pickle\n\n#pd.set_option('display.max_colwidth', 1000)\n\nimport spacy\nnlp = spacy.load(\"en_core_web_lg\")\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2021-11-15T08:12:34.253754Z","iopub.execute_input":"2021-11-15T08:12:34.254177Z","iopub.status.idle":"2021-11-15T08:12:36.653792Z","shell.execute_reply.started":"2021-11-15T08:12:34.254128Z","shell.execute_reply":"2021-11-15T08:12:36.652858Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"#pd.set_option('display.max_colwidth', None)\n#pd.set_option(\"max_columns\", None)\n#pd.set_option('max_colwidth', None)","metadata":{"execution":{"iopub.status.busy":"2021-11-14T19:32:00.088506Z","iopub.execute_input":"2021-11-14T19:32:00.088834Z","iopub.status.idle":"2021-11-14T19:32:00.093471Z","shell.execute_reply.started":"2021-11-14T19:32:00.088792Z","shell.execute_reply":"2021-11-14T19:32:00.092496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load Pipeline","metadata":{}},{"cell_type":"code","source":"# Text Proccessing\n\nclass TextPreprocessor(BaseEstimator, TransformerMixin):\n    def __init__(self, text_attribute):\n        self.text_attribute = text_attribute\n    \n    def fit(self, X, y=None):\n        return self\n        \n    def transform(self, X, *_):\n        X_copy = X.copy()\n        return X_copy[self.text_attribute].apply(self._preprocess_text)\n    \n    def _preprocess_text(self, text):\n        return self._lemmatize(self._leave_letters_only(self._clean(text)))\n    \n    def _clean(self, text):\n        bad_symbols = '!\"#%&\\'*+,-<=>?[\\\\]^_`{|}~'\n        text_without_symbols = text.translate(str.maketrans('', '', bad_symbols))\n\n        text_without_bad_words = ''\n        for line in text_without_symbols.split('\\n'):\n            if not line.lower().startswith('from:') and not line.lower().endswith('writes:'):\n                text_without_bad_words += line + '\\n'\n\n        clean_text = text_without_bad_words\n        email_regex = r'([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+)'\n        regexes_to_remove = [email_regex, r'Subject:', r'Re:']\n        for r in regexes_to_remove:\n            clean_text = re.sub(r, '', clean_text)\n\n        return clean_text\n    \n    def _leave_letters_only(self, text):\n        text_without_punctuation = text.translate(str.maketrans('', '', string.punctuation))\n        return ' '.join(re.findall(\"[a-zA-Z]+\", text_without_punctuation))\n    \n    def _lemmatize(self, text):\n        doc = nlp(text)\n        words = [x.lemma_ for x in [y for y in doc if not y.is_stop and y.pos_ != 'PUNCT' \n                                    and y.pos_ != 'PART' and y.pos_ != 'X']]\n        return ' '.join(words)\n    \n\nclass DenseTransformer(TransformerMixin):\n\n    def fit(self, X, y=None, **fit_params):\n        return self\n\n    def transform(self, X, y=None, **fit_params):\n        return X.todense()","metadata":{"execution":{"iopub.status.busy":"2021-11-15T08:12:36.655854Z","iopub.execute_input":"2021-11-15T08:12:36.657276Z","iopub.status.idle":"2021-11-15T08:12:36.673515Z","shell.execute_reply.started":"2021-11-15T08:12:36.657234Z","shell.execute_reply":"2021-11-15T08:12:36.672334Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# load the saved pipleine model\npipeline = load(\"../input/suicide-text-pipeline2/Suicide_text_classification (1).joblib\")","metadata":{"execution":{"iopub.status.busy":"2021-11-15T08:12:36.676150Z","iopub.execute_input":"2021-11-15T08:12:36.676603Z","iopub.status.idle":"2021-11-15T08:12:36.954254Z","shell.execute_reply.started":"2021-11-15T08:12:36.676528Z","shell.execute_reply":"2021-11-15T08:12:36.953352Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"#Twitter client for connecting to Twitter, fetch tweets and get sentiment\n\nclass TwitterClient(object):\n    '''\n    Generic Twitter Class for sentiment analysis.\n    '''\n    def __init__(self):\n        '''\n        Class constructor or initialization method.\n        '''\n        # keys and tokens from the Twitter Dev Console\n        consumer_key = 'xeBnocCW5BUdWkf8nxV4yCdmq'\n        consumer_secret = 'r0IwBH4VV0F8AH7GGXfJ6FueWt5Cc1VstEzcKyVH2Rhn4mBJZj'\n        access_token = '336239701-ELhAtnZciTmNwDrkYAlv3EmS58xfjpWY6JAKD7ik'\n        access_token_secret = 'iaVZc0rZaDSBQonLXpiwnGD3KLPXk1OVf3Q50YHYJlV8x'\n        \n        # attempt authentication\n        try:\n            # create OAuthHandler object\n            self.auth = OAuthHandler(consumer_key, consumer_secret)\n            # set access token and secret\n            self.auth.set_access_token(access_token, access_token_secret)\n            # create tweepy API object to fetch tweets\n            self.api = tweepy.API(self.auth)\n            self.api.verify_credentials()\n            print(\"Authentication OK!\")\n        except:\n            print(\"Error: Authentication Failed\")\n\n    def clean_tweet(self, tweet):\n        '''\n        Utility function to clean tweet text by removing links, special characters\n        using simple regex statements.\n        '''\n        return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n\n    def get_tweet_sentiment(self, tweet):\n        '''\n        Utility function to classify sentiment of passed tweet\n        using textblob's sentiment method\n        '''\n        # create TextBlob object of passed tweet text\n        analysis = TextBlob(self.clean_tweet(tweet))\n        # set sentiment\n        if analysis.sentiment.polarity > 0:\n            return 'positive'\n        elif analysis.sentiment.polarity == 0:\n            return 'neutral'\n        else:\n            return 'negative'\n\n    def get_tweets(self, query, count, lang, locale):\n        '''\n        Main function to fetch tweets and parse them.\n        '''\n        # empty list to store parsed tweets\n        tweets = []\n\n        try:\n            # call twitter api to fetch tweets\n            fetched_tweets = self.api.search_tweets(q = query, count = count, lang = lang, locale = locale)\n\n            # parsing tweets one by one\n            for tweet in fetched_tweets:\n                # empty dictionary to store required params of a tweet\n                parsed_tweet = {}\n\n                # saving text of tweet\n                parsed_tweet['text'] = tweet.text\n                # saving sentiment of tweet\n                parsed_tweet['sentiment'] = self.get_tweet_sentiment(tweet.text)\n\n                # appending parsed tweet to tweets list\n                if tweet.retweet_count > 0:\n                    # if tweet has retweets, ensure that it is appended only once\n                    if parsed_tweet not in tweets:\n                        tweets.append(parsed_tweet)\n                else:\n                    tweets.append(parsed_tweet)\n\n            # return parsed tweets\n            return tweets\n\n        except tweepy.TweepError as e:\n            # print error (if any)\n            print(\"Error : \" + str(e))\n","metadata":{"execution":{"iopub.status.busy":"2021-11-15T08:12:36.955649Z","iopub.execute_input":"2021-11-15T08:12:36.955983Z","iopub.status.idle":"2021-11-15T08:12:36.972750Z","shell.execute_reply.started":"2021-11-15T08:12:36.955941Z","shell.execute_reply":"2021-11-15T08:12:36.971785Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"## Get Tweet & Predict","metadata":{}},{"cell_type":"code","source":"import json\nimport plotly\nimport plotly.express as px\n\ndef main(q):\n    # creating object of TwitterClient Class\n    api = TwitterClient()\n    # calling function to get tweets\n    global tweets, tweets_df, graphJSON\n    #q = input(\"Enter text to predict :\")\n    tweets = api.get_tweets(query = q, count = 200, lang = \"en\", locale = \"ja\")\n    \n    tweets_df = pd.DataFrame(tweets)\n    tweets_text = tweets_df.drop(columns = [\"sentiment\"])\n    \n    pred = pipeline.predict(tweets_text)\n    \n    tweets_df[\"prediction\"] = pred\n    \n    #pie chart of world suicide rate by sex 1985-2016\n\n    data = tweets_df.prediction.value_counts()\n    labels = [\"non-suicide\", \"suicide\"]\n    colors = sns.color_palette('pastel')[0:5]\n    \n\n    fig = px.pie(data, values = data, names= labels)\n    #fig.show()\n    graphJSON = json.dumps(fig, cls=plotly.utils.PlotlyJSONEncoder)\n    \n    #print(tweets_df)\n    return tweets_df\n\n    \n\n#if __name__ == \"__main__\":\n    # calling main function\n    #main()\n","metadata":{"execution":{"iopub.status.busy":"2021-11-15T08:14:16.355556Z","iopub.execute_input":"2021-11-15T08:14:16.356412Z","iopub.status.idle":"2021-11-15T08:14:16.365371Z","shell.execute_reply.started":"2021-11-15T08:14:16.356374Z","shell.execute_reply":"2021-11-15T08:14:16.364476Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Deploy Using Flask","metadata":{}},{"cell_type":"code","source":"#pip freeze > requirements.txt","metadata":{"execution":{"iopub.status.busy":"2021-11-14T19:32:02.218978Z","iopub.execute_input":"2021-11-14T19:32:02.219211Z","iopub.status.idle":"2021-11-14T19:32:02.222482Z","shell.execute_reply.started":"2021-11-14T19:32:02.219183Z","shell.execute_reply":"2021-11-14T19:32:02.221758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install flask-ngrok","metadata":{"execution":{"iopub.status.busy":"2021-11-15T08:14:58.091329Z","iopub.execute_input":"2021-11-15T08:14:58.091615Z","iopub.status.idle":"2021-11-15T08:15:07.289988Z","shell.execute_reply.started":"2021-11-15T08:14:58.091586Z","shell.execute_reply":"2021-11-15T08:15:07.289095Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"#importing the required libraries\nfrom flask import Flask, render_template, request, redirect, url_for\nfrom joblib import load\nfrom flask_ngrok import run_with_ngrok\n#from get_tweets import get_related_tweets\nfrom logging import FileHandler,WARNING","metadata":{"execution":{"iopub.status.busy":"2021-11-15T08:15:07.292558Z","iopub.execute_input":"2021-11-15T08:15:07.292919Z","iopub.status.idle":"2021-11-15T08:15:07.576114Z","shell.execute_reply.started":"2021-11-15T08:15:07.292870Z","shell.execute_reply":"2021-11-15T08:15:07.575109Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"#set display option\npd.set_option('display.max_rows', 200)\n# start flask\napp = Flask(__name__, template_folder = \"../input/templates\")\nrun_with_ngrok(app)\n#app.debug = True\n\n# render default webpage\n@app.route('/')\ndef home():\n    return render_template('index.html')\n\n# when the post method detect, then redirect to success function\n@app.route('/', methods=['POST', 'GET'])\ndef get_data():\n    if request.method == 'POST':\n        user = request.form['text']\n        return redirect(url_for('success', name=user))\n\n# get the data for the requested query\n@app.route('/success/<name>')\ndef success(name):\n    results = main(name)\n    \n    return render_template('simple.html', tables=[results.to_html(classes='data')], titles= results.columns.values, graphJSON=graphJSON)\n#\"<xmp>\" + str(main(name)) + \" </xmp> \"\nif __name__ == '__main__':\n    app.run() ","metadata":{"execution":{"iopub.status.busy":"2021-11-15T09:02:49.585014Z","iopub.execute_input":"2021-11-15T09:02:49.585469Z","iopub.status.idle":"2021-11-15T09:05:48.036226Z","shell.execute_reply.started":"2021-11-15T09:02:49.585429Z","shell.execute_reply":"2021-11-15T09:05:48.035401Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}